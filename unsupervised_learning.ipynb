{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kshpnrMtM65H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generating sample data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "salary = np.random.normal(loc=50000, scale=15000, size=n_samples).reshape(-1, 1)  # Assuming salary follows a normal distribution\n",
        "data = pd.DataFrame(salary, columns=['Salary'])\n",
        "\n",
        "# Scaling the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "# DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(scaled_data)\n",
        "\n",
        "# Adding labels to the original data\n",
        "data['KMeans_Labels'] = kmeans_labels\n",
        "data['DBSCAN_Labels'] = dbscan_labels\n",
        "\n",
        "# Visualizing clusters\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(data['Salary'], np.zeros_like(data['Salary']), c=kmeans_labels, cmap='viridis')\n",
        "plt.title('K-means Clustering')\n",
        "plt.xlabel('Salary')\n",
        "plt.yticks([])\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(data['Salary'], np.zeros_like(data['Salary']), c=dbscan_labels, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering')\n",
        "plt.xlabel('Salary')\n",
        "plt.yticks([])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating sample customer IDs\n",
        "customer_ids = ['Cust' + str(i) for i in range(n_samples)]\n",
        "\n",
        "# Adding customer IDs to the data\n",
        "data['Customer_ID'] = customer_ids\n",
        "\n",
        "# Creating pivot tables for K-means and DBSCAN labels\n",
        "kmeans_pivot_table = pd.pivot_table(data, values='Customer_ID', index='KMeans_Labels', aggfunc=list)\n",
        "dbscan_pivot_table = pd.pivot_table(data, values='Customer_ID', index='DBSCAN_Labels', aggfunc=list)\n",
        "\n",
        "print(\"K-means Pivot Table:\")\n",
        "print(kmeans_pivot_table)\n",
        "print(\"\\nDBSCAN Pivot Table:\")\n",
        "print(dbscan_pivot_table)\n"
      ],
      "metadata": {
        "id": "r9vFacT5NDSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "SG5aTA8YNWwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='KMeans_Labels', y='Salary', data=data)\n",
        "plt.title('Box plot of Salary by K-means Clusters')\n",
        "plt.xlabel('Cluster Label')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OB5fHWrFNUa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for label in sorted(data['KMeans_Labels'].unique()):\n",
        "    sns.histplot(data[data['KMeans_Labels'] == label]['Salary'], kde=True, label=f'Cluster {label}')\n",
        "plt.title('Histogram of Salary by K-means Clusters')\n",
        "plt.xlabel('Salary')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "euXVMhQsNdqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for label in sorted(data['KMeans_Labels'].unique()):\n",
        "    sns.histplot(data[data['KMeans_Labels'] == label]['Salary'], kde=True, label=f'Cluster {label}')\n",
        "plt.title('Histogram of Salary by K-means Clusters')\n",
        "plt.xlabel('Salary')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AQqL2vhFNq42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generating sample data (assuming two features: amount and frequency of transactions)\n",
        "np.random.seed(42)\n",
        "n_normal_transactions = 1000\n",
        "n_fraud_transactions = 50\n",
        "\n",
        "normal_transactions = np.random.normal(loc=[100, 10], scale=[20, 5], size=(n_normal_transactions, 2))\n",
        "fraud_transactions = np.random.normal(loc=[400, 100], scale=[50, 20], size=(n_fraud_transactions, 2))\n",
        "\n",
        "# Combining normal and fraud transactions\n",
        "transactions = np.vstack((normal_transactions, fraud_transactions))\n",
        "labels = np.array([0] * n_normal_transactions + [1] * n_fraud_transactions)  # 0 for normal, 1 for fraud\n",
        "\n",
        "# Scaling the data\n",
        "scaler = StandardScaler()\n",
        "scaled_transactions = scaler.fit_transform(transactions)\n",
        "\n",
        "# DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan.fit(scaled_transactions)\n",
        "predicted_labels = dbscan.labels_\n",
        "\n",
        "# Visualizing clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(scaled_transactions[:, 0], scaled_transactions[:, 1], c=predicted_labels, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering for Fraud Detection')\n",
        "plt.xlabel('Scaled Amount of Transaction')\n",
        "plt.ylabel('Scaled Frequency of Transaction')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()\n",
        "\n",
        "# Identify potential fraud transactions\n",
        "potential_fraud_indices = np.where(predicted_labels == -1)[0]\n",
        "potential_fraud_transactions = transactions[potential_fraud_indices]\n",
        "\n",
        "print(\"Potential Fraud Transactions:\")\n",
        "print(potential_fraud_transactions)\n"
      ],
      "metadata": {
        "id": "0dn3axQtY2rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate sample traffic data\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulated traffic data with features: hour of the day, route length, and average speed\n",
        "n_samples = 1000\n",
        "traffic_data = pd.DataFrame({\n",
        "    'hour_of_day': np.random.randint(low=0, high=24, size=n_samples),\n",
        "    'route_length': np.random.uniform(low=5, high=20, size=n_samples),  # in miles\n",
        "    'average_speed': np.random.randint(low=20, high=70, size=n_samples)  # in mph\n",
        "})\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_traffic_data = scaler.fit_transform(traffic_data)\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans.fit(scaled_traffic_data)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Visualize the clusters\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(traffic_data['hour_of_day'], traffic_data['average_speed'], c=labels, cmap='viridis', s=10)\n",
        "plt.title('Hour of Day vs Average Speed')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Average Speed')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(traffic_data['route_length'], traffic_data['average_speed'], c=labels, cmap='viridis', s=10)\n",
        "plt.title('Route Length vs Average Speed')\n",
        "plt.xlabel('Route Length (miles)')\n",
        "plt.ylabel('Average Speed')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze cluster centers\n",
        "cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
        "cluster_centers_df = pd.DataFrame(cluster_centers, columns=['hour_of_day', 'route_length', 'average_speed'])\n",
        "print(\"Cluster Centers:\")\n",
        "print(cluster_centers_df)\n"
      ],
      "metadata": {
        "id": "5J1gQE-mZjyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate sample customer data\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulated customer data with features: age, income, and spending score\n",
        "n_samples = 1000\n",
        "customer_data = pd.DataFrame({\n",
        "    'age': np.random.randint(low=18, high=70, size=n_samples),\n",
        "    'income': np.random.normal(loc=50000, scale=20000, size=n_samples),\n",
        "    'spending_score': np.random.randint(low=1, high=101, size=n_samples)\n",
        "})\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_customer_data = scaler.fit_transform(customer_data)\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(scaled_customer_data)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Analyze cluster centers\n",
        "cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
        "cluster_centers_df = pd.DataFrame(cluster_centers, columns=['age', 'income', 'spending_score'])\n",
        "\n",
        "# Identify potential customers within each segment\n",
        "segmented_customers = customer_data.copy()\n",
        "segmented_customers['segment'] = labels\n",
        "\n",
        "# Potential customers within each segment (e.g., customers with high spending score)\n",
        "potential_customers = {}\n",
        "for segment in range(kmeans.n_clusters):\n",
        "    segment_data = segmented_customers[segmented_customers['segment'] == segment]\n",
        "    potential_customers[segment] = segment_data\n",
        "\n",
        "# Print potential customers within each segment\n",
        "for segment, segment_data in potential_customers.items():\n",
        "    print(f\"Potential customers in segment {segment}:\")\n",
        "    print(segment_data.head())  # Adjust this to show more or fewer potential customers as needed\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "LblrtUsfZ--9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate sample customer data\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulated customer data with features: age, income, and spending score\n",
        "n_samples = 1000\n",
        "customer_data = pd.DataFrame({\n",
        "    'age': np.random.randint(low=18, high=70, size=n_samples),\n",
        "    'income': np.random.normal(loc=50000, scale=20000, size=n_samples),\n",
        "    'spending_score': np.random.randint(low=1, high=101, size=n_samples)\n",
        "})\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_customer_data = scaler.fit_transform(customer_data)\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(scaled_customer_data)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Analyze cluster centers\n",
        "cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
        "cluster_centers_df = pd.DataFrame(cluster_centers, columns=['age', 'income', 'spending_score'])\n",
        "\n",
        "# Identify potential customers within each segment\n",
        "segmented_customers = customer_data.copy()\n",
        "segmented_customers['segment'] = labels\n",
        "\n",
        "# Potential customers within each segment (e.g., customers with high spending score)\n",
        "potential_customers = {}\n",
        "for segment in range(kmeans.n_clusters):\n",
        "    segment_data = segmented_customers[segmented_customers['segment'] == segment]\n",
        "    potential_customers[segment] = segment_data\n",
        "\n",
        "# Visualize potential customers within each segment\n",
        "plt.figure(figsize=(12, 6))\n",
        "for segment, segment_data in potential_customers.items():\n",
        "    plt.scatter(segment_data['age'], segment_data['income'], label=f'Segment {segment}', alpha=0.7)\n",
        "\n",
        "plt.scatter(cluster_centers_df['age'], cluster_centers_df['income'], color='black', marker='x', label='Cluster Centers')\n",
        "plt.title('Customer Segmentation with Potential Customers')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Income')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Pqo7pGTjaRCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate random earthquake data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate random latitude and longitude within ranges\n",
        "latitude = np.random.uniform(low=-90, high=90, size=n_samples)\n",
        "longitude = np.random.uniform(low=-180, high=180, size=n_samples)\n",
        "\n",
        "# Generate random magnitudes\n",
        "magnitude = np.random.uniform(low=2.0, high=7.0, size=n_samples)\n",
        "\n",
        "# Create a DataFrame to store the earthquake data\n",
        "earthquake_data = pd.DataFrame({'Latitude': latitude, 'Longitude': longitude, 'Magnitude': magnitude})\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans.fit(earthquake_data[['Latitude', 'Longitude']])\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Visualize the clusters on a map\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(earthquake_data['Longitude'], earthquake_data['Latitude'], c=labels, cmap='viridis', s=10)\n",
        "plt.title('Earthquake Clusters')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QXg92pc0cSYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from networkx.algorithms import community\n",
        "\n",
        "# Load social network data (example: edge list)\n",
        "# You can replace this with your own social network data\n",
        "G = nx.karate_club_graph()\n",
        "\n",
        "# Perform community detection using Louvain method\n",
        "communities = community.greedy_modularity_communities(G)\n",
        "\n",
        "# Plot the social network graph with communities color-coded\n",
        "plt.figure(figsize=(10, 6))\n",
        "pos = nx.spring_layout(G)  # Position nodes using Fruchterman-Reingold force-directed algorithm\n",
        "nx.draw(G, pos, node_color='lightblue', with_labels=True, node_size=300)\n",
        "for idx, com in enumerate(communities):\n",
        "    nx.draw_networkx_nodes(G, pos, nodelist=list(com), node_color=f'C{idx}', node_size=300)\n",
        "plt.title('Social Network with Community Detection')\n",
        "plt.show()\n",
        "\n",
        "# Print communities\n",
        "for idx, com in enumerate(communities):\n",
        "    print(f\"Community {idx}: {list(com)}\")\n"
      ],
      "metadata": {
        "id": "-dgXgUYNcriV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}